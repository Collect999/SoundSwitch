{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOb8n8p1FvoteTRUlYy55aG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AceCentre/SoundSwitch/blob/main/SoundDetect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Clips**\n",
        "The positive_clips should contain audio samples of the sound you are interested in detecting. For example, if you are building a system to detect the sound of a specific word being spoken, your positive clips would contain various instances of that word being spoken by different people, in different tones, and possibly with background noise.\n",
        "\n",
        "**Negative Clips**\n",
        "The negative_clips should contain audio samples that are representative of the types of sounds that the system will encounter but should not react to. This could include background noise, other words being spoken, por any other sounds that are not the target sound. These clips are used to test the system's ability to correctly identify non-target sounds as negative.\n",
        "\n",
        "**Template Clips**\n",
        "\n",
        "\n",
        "The templates are pre-recorded audio clips that are used as a basis for comparison with incoming audio data. These could be the clearest examples of the sound you are trying to detect. In your code, these are loaded from files named Heather1.wav and Heather2.wav. The Mel spectrograms of these templates are computed and stored in S1 and S2.\n",
        "\n"
      ],
      "metadata": {
        "id": "2lWHckCQn8Gs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyWPgF04FGo5",
        "outputId": "d92ddb2a-984a-44c8-b70f-6529bf592b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastdtw in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fastdtw) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastdtw\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "import time\n",
        "import os\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load clips positive and negative\n",
        "\n",
        "def load_clips(folder):\n",
        "    clips = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            filepath = os.path.join(folder, filename)\n",
        "            audio, _ = librosa.load(filepath, sr=44100)\n",
        "            clips.append(audio)\n",
        "    return clips\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "negative_clips = load_clips(\"/content/drive/My Drive/SoundDetectSamples/Background Clips\")\n",
        "positive_clips = load_clips(\"/content/drive/My Drive/SoundDetectSamples/Loud Clips\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFCGPwj6PIpZ",
        "outputId": "73907bc4-b57b-4802-acd4-52ede36045d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_method(method, positive_clips, background_noises, *args):\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "    true_negatives = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Test on positive clips\n",
        "    for clip in positive_clips:\n",
        "        if method(clip, *args):\n",
        "            true_positives += 1\n",
        "        else:\n",
        "            false_negatives += 1\n",
        "\n",
        "    # Test on background noise clips\n",
        "    for noise in background_noises:\n",
        "        if method(noise, *args):\n",
        "            false_positives += 1\n",
        "        else:\n",
        "            true_negatives += 1\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Method: {method.__name__}\")\n",
        "    print(f\"True Positives: {true_positives}\")\n",
        "    print(f\"False Negatives: {false_negatives}\")\n",
        "    print(f\"True Negatives: {true_negatives}\")\n",
        "    print(f\"False Positives: {false_positives}\")\n",
        "    print(f\"Time taken: {elapsed_time} seconds\")\n"
      ],
      "metadata": {
        "id": "jRxaCm3WFd4R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First function uses Mel spectrograms for feature extraction and Dynamic Time Warping (DTW) for comparing the features. The primary steps in both methods are:\n",
        "\n",
        "Compute the Mel spectrogram of the audio clip.\n",
        "Use DTW to find the minimum distance between the Mel spectrogram of the audio clip and the Mel spectrograms of the templates.\n",
        "Compare the minimum distance to a threshold to make a classification decision."
      ],
      "metadata": {
        "id": "rT3_k4-opGZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_correlation_detect(positive_clips, negative_clips, templates, threshold=0.8):\n",
        "    results = {\n",
        "        'true_positives': 0,\n",
        "        'false_negatives': 0,\n",
        "        'true_negatives': 0,\n",
        "        'false_positives': 0\n",
        "    }\n",
        "\n",
        "    def detect_sound(audio_signal, templates, threshold):\n",
        "        #print(f\"Audio Signal Shape: {audio_signal.shape}, Type: {type(audio_signal)}\")\n",
        "        max_correlations = []\n",
        "        for template in templates:\n",
        "            #print(f\"Template Shape: {template.shape}, Type: {type(template)}\")\n",
        "            c = np.correlate(audio_signal, template, mode='valid')\n",
        "            max_correlations.append(np.max(c))\n",
        "        max_correlation = np.max(max_correlations)\n",
        "        return max_correlation > threshold\n",
        "\n",
        "\n",
        "    # Test on positive clips\n",
        "    for clip in positive_clips:\n",
        "        if detect_sound(clip, templates, threshold):\n",
        "            results['true_positives'] += 1\n",
        "        else:\n",
        "            results['false_negatives'] += 1\n",
        "\n",
        "    # Test on negative clips\n",
        "    for clip in negative_clips:\n",
        "        if detect_sound(clip, templates, threshold):\n",
        "            results['false_positives'] += 1\n",
        "        else:\n",
        "            results['true_negatives'] += 1\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "jNTLtj3AfxvA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def melspectrogram_detect(positive_clips, background_noises, templates, sr=44100, threshold=200):\n",
        "    results = {\n",
        "        'true_positives': 0,\n",
        "        'false_negatives': 0,\n",
        "        'true_negatives': 0,\n",
        "        'false_positives': 0\n",
        "    }\n",
        "\n",
        "    # Function to detect sound using Mel spectrogram and DTW\n",
        "    def detect_sound(audio_signal, templates, sr, threshold):\n",
        "        S = librosa.feature.melspectrogram(y=audio_signal, sr=sr, n_mels=128)\n",
        "        min_distance = float('inf')\n",
        "\n",
        "        for template in templates:\n",
        "            distance, _ = fastdtw(S.T, template.T, dist=euclidean)\n",
        "            min_distance = min(min_distance, distance)\n",
        "            #print(f\"Min distance: {min_distance}\")\n",
        "\n",
        "\n",
        "        return min_distance < threshold\n",
        "\n",
        "    # Test on positive clips\n",
        "    for clip in positive_clips:\n",
        "        if detect_sound(clip, templates, sr, threshold):\n",
        "            results['true_positives'] += 1\n",
        "        else:\n",
        "            results['false_negatives'] += 1\n",
        "\n",
        "    for noise in negative_clips:\n",
        "        if detect_sound(noise, templates, sr, threshold):\n",
        "            results['false_positives'] += 1\n",
        "        else:\n",
        "            results['true_negatives'] += 1\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "sokRS9QJROgx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " MFCCs (Mel-Frequency Cepstral Coefficients). MFCCs are often used in speech and audio processing to capture the timbral texture of the audio.  we use MFCCs instead of Mel spectrograms for feature extraction. We also use Euclidean distance for comparison instead of DTW. This should provide a different perspective on the performance of sound detection techniques."
      ],
      "metadata": {
        "id": "GJ21Jo53pKlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mfcc_detect(positive_clips, background_noises, templates, sr=44100, threshold=100):\n",
        "    results = {\n",
        "        'true_positives': 0,\n",
        "        'false_negatives': 0,\n",
        "        'true_negatives': 0,\n",
        "        'false_positives': 0\n",
        "    }\n",
        "\n",
        "    # Function to detect sound using MFCC and Euclidean distance\n",
        "    def detect_sound(audio_signal, templates, sr, threshold):\n",
        "        mfccs = librosa.feature.mfcc(y=audio_signal, sr=sr, n_mfcc=13)\n",
        "        min_distance = float('inf')\n",
        "\n",
        "        for template in templates:\n",
        "            if mfccs.shape[1] != template.shape[1]:\n",
        "                continue  # Skip this template if dimensions don't match\n",
        "            distance = np.sum(euclidean_distances(mfccs.T, template.T))\n",
        "            min_distance = min(min_distance, distance)\n",
        "\n",
        "        return min_distance < threshold\n",
        "\n",
        "    # Test on positive clips\n",
        "    for clip in positive_clips:\n",
        "        if detect_sound(clip, templates, sr, threshold):\n",
        "            results['true_positives'] += 1\n",
        "        else:\n",
        "            results['false_negatives'] += 1\n",
        "\n",
        "    # Test on background noise clips\n",
        "    for noise in background_noises:\n",
        "        if detect_sound(noise, templates, sr, threshold):\n",
        "            results['false_positives'] += 1\n",
        "        else:\n",
        "            results['true_negatives'] += 1\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "et4Fgjm_o13C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this version, we use MFCCs as features for the SVM classifier. We train the classifier using the mean MFCCs across time for each clip, labeling positive clips as 1 and negative clips as 0. After training, we test the classifier on both positive and negative clips and update the results dictionary accordingly."
      ],
      "metadata": {
        "id": "6ofjtsD-XGcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def svm_detect(positive_clips, background_noises, sr=44100):\n",
        "    results = {\n",
        "        'true_positives': 0,\n",
        "        'false_negatives': 0,\n",
        "        'true_negatives': 0,\n",
        "        'false_positives': 0\n",
        "    }\n",
        "\n",
        "    # Extract MFCC features for training\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for clip in positive_clips:\n",
        "        mfccs = librosa.feature.mfcc(y=clip, sr=sr, n_mfcc=13)\n",
        "        X_train.append(mfccs.mean(axis=1))\n",
        "        y_train.append(1)  # Label for positive clips\n",
        "\n",
        "    for noise in background_noises:\n",
        "        mfccs = librosa.feature.mfcc(y=noise, sr=sr, n_mfcc=13)\n",
        "        X_train.append(mfccs.mean(axis=1))\n",
        "        y_train.append(0)  # Label for negative clips\n",
        "\n",
        "    # Train the SVM classifier\n",
        "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Test on positive clips\n",
        "    for clip in positive_clips:\n",
        "        mfccs = librosa.feature.mfcc(y=clip, sr=sr, n_mfcc=13)\n",
        "        prediction = clf.predict([mfccs.mean(axis=1)])\n",
        "        if prediction == 1:\n",
        "            results['true_positives'] += 1\n",
        "        else:\n",
        "            results['false_negatives'] += 1\n",
        "\n",
        "    # Test on background noise clips\n",
        "    for noise in background_noises:\n",
        "        mfccs = librosa.feature.mfcc(y=noise, sr=sr, n_mfcc=13)\n",
        "        prediction = clf.predict([mfccs.mean(axis=1)])\n",
        "        if prediction == 0:\n",
        "            results['true_negatives'] += 1\n",
        "        else:\n",
        "            results['false_positives'] += 1\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "FvcVP013WlUQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined technique\n",
        "\n",
        "def extract_combined_features(audio, sr):\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
        "\n",
        "    # Take the mean of each feature to use as the feature vector\n",
        "    mfccs_mean = np.mean(mfccs, axis=1)\n",
        "    chroma_mean = np.mean(chroma, axis=1)\n",
        "    spectral_contrast_mean = np.mean(spectral_contrast, axis=1)\n",
        "\n",
        "    # Combine the features into a single array\n",
        "    combined_features = np.concatenate((mfccs_mean, chroma_mean, spectral_contrast_mean))\n",
        "\n",
        "    return combined_features\n",
        "\n",
        "def svm_detect_combined(positive_clips, background_noises, sr=44100):\n",
        "    results = {\n",
        "        'true_positives': 0,\n",
        "        'false_negatives': 0,\n",
        "        'true_negatives': 0,\n",
        "        'false_positives': 0\n",
        "    }\n",
        "\n",
        "    # Check if positive_clips and background_noises have data\n",
        "    if len(positive_clips) == 0 or len(background_noises) == 0:\n",
        "        print(\"Either positive_clips or background_noises is empty. Please check your data.\")\n",
        "        return results  # You can choose to return an empty results dict or handle it differently\n",
        "\n",
        "    # Extract features for training\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for clip in positive_clips:\n",
        "        features = extract_combined_features(clip, sr)\n",
        "        X_train.append(features)\n",
        "        y_train.append(1)  # Label for positive clips\n",
        "\n",
        "    for noise in background_noises:\n",
        "        features = extract_combined_features(noise, sr)\n",
        "        X_train.append(features)\n",
        "        y_train.append(0)  # Label for negative clips\n",
        "\n",
        "    # Train the SVM classifier\n",
        "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Test on positive clips\n",
        "    for clip in positive_clips:\n",
        "        features = extract_combined_features(clip, sr)\n",
        "        prediction = clf.predict([features])\n",
        "        if prediction == 1:\n",
        "            results['true_positives'] += 1\n",
        "        else:\n",
        "            results['false_negatives'] += 1\n",
        "\n",
        "    # Test on background noise clips\n",
        "    for noise in background_noises:\n",
        "        features = extract_combined_features(noise, sr)\n",
        "        prediction = clf.predict([features])\n",
        "        if prediction == 0:\n",
        "            results['true_negatives'] += 1\n",
        "        else:\n",
        "            results['false_positives'] += 1\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "P1QN5HZsQJex"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-recorded templates and compute their Mel spectrograms\n",
        "audio1, sr = librosa.load(\"/content/drive/My Drive/SoundDetectSamples/Heather1.wav\", sr=44100)\n",
        "audio2, _ = librosa.load(\"/content/drive/My Drive/SoundDetectSamples/Heather2.wav\", sr=44100)\n",
        "\n",
        "S1 = librosa.feature.melspectrogram(y=audio1, sr=sr, n_mels=128)\n",
        "S2 = librosa.feature.melspectrogram(y=audio2, sr=sr, n_mels=128)\n",
        "\n",
        "templates = [S1, S2]\n",
        "\n",
        "def evaluate_results(results):\n",
        "    tp = results['true_positives']\n",
        "    fp = results['false_positives']\n",
        "    fn = results['false_negatives']\n",
        "    tn = results['true_negatives']\n",
        "\n",
        "    # Calculate metrics\n",
        "    try:\n",
        "        precision = tp / (tp + fp)\n",
        "    except ZeroDivisionError:\n",
        "        precision = 0.0\n",
        "\n",
        "    try:\n",
        "        recall = tp / (tp + fn)\n",
        "    except ZeroDivisionError:\n",
        "        recall = 0.0\n",
        "\n",
        "    try:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        f1_score = 0.0\n",
        "\n",
        "    try:\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    except ZeroDivisionError:\n",
        "        accuracy = 0.0\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1_score:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "templates_raw = [audio1, audio2]\n",
        "cross_corr_results = cross_correlation_detect(positive_clips, negative_clips, templates_raw, threshold=0.8)\n",
        "\n",
        "# Evaluate and print metrics\n",
        "print(\"Metrics for Cross-Correlation Detection:\")\n",
        "evaluate_results(cross_corr_results)\n",
        "\n",
        "# Assuming you've run your tests and have results for each\n",
        "mel_results = melspectrogram_detect(positive_clips, negative_clips, templates)\n",
        "\n",
        "audio1, sr = librosa.load(\"/content/drive/My Drive/SoundDetectSamples/Heather1.wav\", sr=44100)\n",
        "audio2, _ = librosa.load(\"/content/drive/My Drive/SoundDetectSamples/Heather2.wav\", sr=44100)\n",
        "M1 = librosa.feature.mfcc(y=audio1, sr=sr, n_mfcc=13)\n",
        "M2 = librosa.feature.mfcc(y=audio2, sr=sr, n_mfcc=13)\n",
        "mfcc_templates = [M1, M2]\n",
        "\n",
        "mcfc_results = mfcc_detect(positive_clips, negative_clips, templates)\n",
        "svm_results = svm_detect(positive_clips, negative_clips)\n",
        "\n",
        "# Evaluate and print metrics for each method\n",
        "print(\"\\nMetrics for Mel Spectrogram Detection:\")\n",
        "evaluate_results(mel_results)\n",
        "\n",
        "print(\"\\nMetrics for MCFC Detection:\")\n",
        "evaluate_results(mcfc_results)\n",
        "\n",
        "print(\"\\nMetrics for SVM Detection:\")\n",
        "evaluate_results(svm_results)\n",
        "\n",
        "# Assuming you've run your tests and have results for each\n",
        "svm_combined_results = svm_detect_combined(positive_clips, negative_clips)\n",
        "\n",
        "# Evaluate and print metrics for the combined SVM method\n",
        "print(\"\\nMetrics for Combined SVM Detection:\")\n",
        "evaluate_results(svm_combined_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4mlq4gmHwJC",
        "outputId": "e7f7fd12-898b-425b-8961-25c48a9a0fc1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics for Cross-Correlation Detection:\n",
            "Precision: 0.5000\n",
            "Recall: 1.0000\n",
            "F1 Score: 0.6667\n",
            "Accuracy: 0.5000\n",
            "\n",
            "Metrics for Mel Spectrogram Detection:\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1 Score: 0.0000\n",
            "Accuracy: 0.5000\n",
            "\n",
            "Metrics for MCFC Detection:\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1 Score: 0.0000\n",
            "Accuracy: 0.5000\n",
            "\n",
            "Metrics for SVM Detection:\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n",
            "Accuracy: 1.0000\n",
            "\n",
            "Metrics for Combined SVM Detection:\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n",
            "Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do the scores mean?\n",
        "\n",
        "Precision: 1.0000\n",
        "Precision is the ratio of true positive predictions to the total number of positive predictions made (true positives + false positives). A precision score of 1.0000 means that every time the model predicted that a clip was positive, it was correct. In other words, there were no false positives.\n",
        "\n",
        "Recall: 0.9000\n",
        "Recall is the ratio of true positive predictions to the total number of actual positive instances (true positives + false negatives). A recall score of 0.9000 means that the model correctly identified 90% of all actual positive clips. The remaining 10% were false negatives, meaning the model incorrectly classified them as negative.\n",
        "\n",
        "F1 Score: 0.9474\n",
        "The F1 Score is the harmonic mean of Precision and Recall and provides a single score that balances the trade-off between Precision and Recall. The F1 Score ranges between 0 and 1, where 1 indicates perfect precision and recall, and 0 is the worst score. An F1 Score of 0.9474 is quite high, indicating that the model has both good precision and good recall.\n",
        "\n",
        "Accuracy: 0.9500\n",
        "Accuracy is the ratio of correct predictions (both true positives and true negatives) to the total number of instances (true positives + true negatives + false positives + false negatives). An accuracy of 0.9500 means that the model correctly classified 95% of all clips, whether they were positive or negative.\n",
        "\n",
        "In summary, these metrics suggest that your SVM-based model is performing very well. It's correctly identifying almost all of the positive and negative clips, and when it predicts a clip is positive, it's always correct. However, it's missing 10% of the actual positive clips, as indicated by the recall of 0.9000."
      ],
      "metadata": {
        "id": "RMnCjlOVjSIY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tm-rb38WQ-v8"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}