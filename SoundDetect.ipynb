{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSXrUcbjOD3jvK7+yBMJkJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AceCentre/SoundSwitch/blob/main/SoundDetect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Clips**\n",
        "The positive_clips should contain audio samples of the sound you are interested in detecting. For example, if you are building a system to detect the sound of a specific word being spoken, your positive clips would contain various instances of that word being spoken by different people, in different tones, and possibly with background noise.\n",
        "\n",
        "**Negative Clips**\n",
        "The negative_clips should contain audio samples that are representative of the types of sounds that the system will encounter but should not react to. This could include background noise, other words being spoken, or any other sounds that are not the target sound. These clips are used to test the system's ability to correctly identify non-target sounds as negative.\n",
        "\n",
        "**Template Clips**\n",
        "The templates are pre-recorded audio clips that are used as a basis for comparison with incoming audio data. These could be the clearest examples of the sound you are trying to detect. In your code, these are loaded from files named Heather1.wav and Heather2.wav. The Mel spectrograms of these templates are computed and stored in S1 and S2."
      ],
      "metadata": {
        "id": "2lWHckCQn8Gs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyWPgF04FGo5",
        "outputId": "b9afc98e-6a83-4773-fa1f-9375afcca774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastdtw\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/133.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m133.1/133.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fastdtw) (1.23.5)\n",
            "Building wheels for collected packages: fastdtw\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp310-cp310-linux_x86_64.whl size=512378 sha256=ceeb4d66d18d5df175954551e4d66a32301e9a6488a8d4d2347341f7ef621704\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/c8/f7/c25448dab74c3acf4848bc25d513c736bb93910277e1528ef4\n",
            "Successfully built fastdtw\n",
            "Installing collected packages: fastdtw\n",
            "Successfully installed fastdtw-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install fastdtw\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "import time\n",
        "import os\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load clips positive and negative\n",
        "\n",
        "def load_clips(folder):\n",
        "    clips = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            filepath = os.path.join(folder, filename)\n",
        "            audio, _ = librosa.load(filepath, sr=44100)\n",
        "            clips.append(audio)\n",
        "    return clips\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "negative_clips = load_clips(\"/content/drive/My Drive/SoundDetectSamples/Background Clips\")\n",
        "positive_clips = load_clips(\"/content/drive/My Drive/SoundDetectSamples/Positive Clips\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFCGPwj6PIpZ",
        "outputId": "c6ba2f53-04f8-4f6b-9196-7a01038abd75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_method(method, positive_clips, background_noises, *args):\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "    true_negatives = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Test on positive clips\n",
        "    for clip in positive_clips:\n",
        "        if method(clip, *args):\n",
        "            true_positives += 1\n",
        "        else:\n",
        "            false_negatives += 1\n",
        "\n",
        "    # Test on background noise clips\n",
        "    for noise in background_noises:\n",
        "        if method(noise, *args):\n",
        "            false_positives += 1\n",
        "        else:\n",
        "            true_negatives += 1\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Method: {method.__name__}\")\n",
        "    print(f\"True Positives: {true_positives}\")\n",
        "    print(f\"False Negatives: {false_negatives}\")\n",
        "    print(f\"True Negatives: {true_negatives}\")\n",
        "    print(f\"False Positives: {false_positives}\")\n",
        "    print(f\"Time taken: {elapsed_time} seconds\")\n"
      ],
      "metadata": {
        "id": "jRxaCm3WFd4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First function uses Mel spectrograms for feature extraction and Dynamic Time Warping (DTW) for comparing the features. The primary steps in both methods are:\n",
        "\n",
        "Compute the Mel spectrogram of the audio clip.\n",
        "Use DTW to find the minimum distance between the Mel spectrogram of the audio clip and the Mel spectrograms of the templates.\n",
        "Compare the minimum distance to a threshold to make a classification decision."
      ],
      "metadata": {
        "id": "rT3_k4-opGZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def melspectrogram_detect(positive_clips, background_noises, templates, sr=44100, threshold=1000):\n",
        "    results = {\n",
        "        'true_positives': 0,\n",
        "        'false_negatives': 0,\n",
        "        'true_negatives': 0,\n",
        "        'false_positives': 0\n",
        "    }\n",
        "\n",
        "    # Function to detect sound using Mel spectrogram and DTW\n",
        "    def detect_sound(audio_signal, templates, sr, threshold):\n",
        "        S = librosa.feature.melspectrogram(y=audio_signal, sr=sr, n_mels=128)\n",
        "        min_distance = float('inf')\n",
        "\n",
        "        for template in templates:\n",
        "            distance, _ = fastdtw(S.T, template.T, dist=euclidean)\n",
        "            min_distance = min(min_distance, distance)\n",
        "            #print(f\"Min distance: {min_distance}\")\n",
        "\n",
        "\n",
        "        return min_distance < threshold\n",
        "\n",
        "    # Test on positive clips\n",
        "    for clip in positive_clips:\n",
        "        if detect_sound(clip, templates, sr, threshold):\n",
        "            results['true_positives'] += 1\n",
        "        else:\n",
        "            results['false_negatives'] += 1\n",
        "\n",
        "    for noise in negative_clips:\n",
        "        if detect_sound(noise, templates, sr, threshold):\n",
        "            results['false_positives'] += 1\n",
        "        else:\n",
        "            results['true_negatives'] += 1\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "sokRS9QJROgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " MFCCs (Mel-Frequency Cepstral Coefficients). MFCCs are often used in speech and audio processing to capture the timbral texture of the audio.  we use MFCCs instead of Mel spectrograms for feature extraction. We also use Euclidean distance for comparison instead of DTW. This should provide a different perspective on the performance of sound detection techniques."
      ],
      "metadata": {
        "id": "GJ21Jo53pKlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mfcc_detect(positive_clips, background_noises, templates, sr=44100, threshold=1000):\n",
        "    results = {\n",
        "        'true_positives': 0,\n",
        "        'false_negatives': 0,\n",
        "        'true_negatives': 0,\n",
        "        'false_positives': 0\n",
        "    }\n",
        "\n",
        "    # Function to detect sound using MFCC and Euclidean distance\n",
        "    def detect_sound(audio_signal, templates, sr, threshold):\n",
        "        mfccs = librosa.feature.mfcc(y=audio_signal, sr=sr, n_mfcc=13)\n",
        "        min_distance = float('inf')\n",
        "\n",
        "        for template in templates:\n",
        "            if mfccs.shape[1] != template.shape[1]:\n",
        "                continue  # Skip this template if dimensions don't match\n",
        "            distance = np.sum(euclidean_distances(mfccs.T, template.T))\n",
        "            min_distance = min(min_distance, distance)\n",
        "\n",
        "        return min_distance < threshold\n",
        "\n",
        "    # Test on positive clips\n",
        "    for clip in positive_clips:\n",
        "        if detect_sound(clip, templates, sr, threshold):\n",
        "            results['true_positives'] += 1\n",
        "        else:\n",
        "            results['false_negatives'] += 1\n",
        "\n",
        "    # Test on background noise clips\n",
        "    for noise in background_noises:\n",
        "        if detect_sound(noise, templates, sr, threshold):\n",
        "            results['false_positives'] += 1\n",
        "        else:\n",
        "            results['true_negatives'] += 1\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "et4Fgjm_o13C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this version, we use MFCCs as features for the SVM classifier. We train the classifier using the mean MFCCs across time for each clip, labeling positive clips as 1 and negative clips as 0. After training, we test the classifier on both positive and negative clips and update the results dictionary accordingly."
      ],
      "metadata": {
        "id": "6ofjtsD-XGcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def svm_detect(positive_clips, background_noises, sr=44100):\n",
        "    results = {\n",
        "        'true_positives': 0,\n",
        "        'false_negatives': 0,\n",
        "        'true_negatives': 0,\n",
        "        'false_positives': 0\n",
        "    }\n",
        "\n",
        "    # Extract MFCC features for training\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for clip in positive_clips:\n",
        "        mfccs = librosa.feature.mfcc(y=clip, sr=sr, n_mfcc=13)\n",
        "        X_train.append(mfccs.mean(axis=1))\n",
        "        y_train.append(1)  # Label for positive clips\n",
        "\n",
        "    for noise in background_noises:\n",
        "        mfccs = librosa.feature.mfcc(y=noise, sr=sr, n_mfcc=13)\n",
        "        X_train.append(mfccs.mean(axis=1))\n",
        "        y_train.append(0)  # Label for negative clips\n",
        "\n",
        "    # Train the SVM classifier\n",
        "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Test on positive clips\n",
        "    for clip in positive_clips:\n",
        "        mfccs = librosa.feature.mfcc(y=clip, sr=sr, n_mfcc=13)\n",
        "        prediction = clf.predict([mfccs.mean(axis=1)])\n",
        "        if prediction == 1:\n",
        "            results['true_positives'] += 1\n",
        "        else:\n",
        "            results['false_negatives'] += 1\n",
        "\n",
        "    # Test on background noise clips\n",
        "    for noise in background_noises:\n",
        "        mfccs = librosa.feature.mfcc(y=noise, sr=sr, n_mfcc=13)\n",
        "        prediction = clf.predict([mfccs.mean(axis=1)])\n",
        "        if prediction == 0:\n",
        "            results['true_negatives'] += 1\n",
        "        else:\n",
        "            results['false_positives'] += 1\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "FvcVP013WlUQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-recorded templates and compute their Mel spectrograms\n",
        "audio1, sr = librosa.load(\"/content/drive/My Drive/SoundDetectSamples/Heather1.wav\", sr=44100)\n",
        "audio2, _ = librosa.load(\"/content/drive/My Drive/SoundDetectSamples/Heather2.wav\", sr=44100)\n",
        "\n",
        "S1 = librosa.feature.melspectrogram(y=audio1, sr=sr, n_mels=128)\n",
        "S2 = librosa.feature.melspectrogram(y=audio2, sr=sr, n_mels=128)\n",
        "\n",
        "templates = [S1, S2]\n",
        "\n",
        "def evaluate_results(results):\n",
        "    tp = results['true_positives']\n",
        "    fp = results['false_positives']\n",
        "    fn = results['false_negatives']\n",
        "    tn = results['true_negatives']\n",
        "\n",
        "    # Calculate metrics\n",
        "    try:\n",
        "        precision = tp / (tp + fp)\n",
        "    except ZeroDivisionError:\n",
        "        precision = 0.0\n",
        "\n",
        "    try:\n",
        "        recall = tp / (tp + fn)\n",
        "    except ZeroDivisionError:\n",
        "        recall = 0.0\n",
        "\n",
        "    try:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        f1_score = 0.0\n",
        "\n",
        "    try:\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    except ZeroDivisionError:\n",
        "        accuracy = 0.0\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1_score:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Assuming you've run your tests and have results for each\n",
        "mel_results = melspectrogram_detect(positive_clips, negative_clips, templates)\n",
        "\n",
        "audio1, sr = librosa.load(\"/content/drive/My Drive/SoundDetectSamples/Heather1.wav\", sr=44100)\n",
        "audio2, _ = librosa.load(\"/content/drive/My Drive/SoundDetectSamples/Heather2.wav\", sr=44100)\n",
        "M1 = librosa.feature.mfcc(y=audio1, sr=sr, n_mfcc=13)\n",
        "M2 = librosa.feature.mfcc(y=audio2, sr=sr, n_mfcc=13)\n",
        "mfcc_templates = [M1, M2]\n",
        "\n",
        "mcfc_results = mfcc_detect(positive_clips, negative_clips, templates)\n",
        "svm_results = svm_detect(positive_clips, negative_clips)\n",
        "\n",
        "# Evaluate and print metrics for each method\n",
        "print(\"Metrics for Mel Spectrogram Detection:\")\n",
        "evaluate_results(mel_results)\n",
        "\n",
        "print(\"\\nMetrics for MCFC Detection:\")\n",
        "evaluate_results(mcfc_results)\n",
        "\n",
        "print(\"\\nMetrics for SVM Detection:\")\n",
        "evaluate_results(svm_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "r4mlq4gmHwJC",
        "outputId": "266ca846-a196-43f3-ed3f-b3780a56c8cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f295194c9e16>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-recorded templates and compute their Mel spectrograms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maudio1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/SoundDetectSamples/Heather1.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maudio2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/SoundDetectSamples/Heather2.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mS1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'librosa' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tm-rb38WQ-v8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}